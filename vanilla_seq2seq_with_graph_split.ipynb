{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# Dependencies\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from preproc_pipe import *\n",
    "\n",
    "#tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_batch(inputs, max_sequence_length=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        inputs:\n",
    "            list of sentences (integer lists)\n",
    "        max_sequence_length:\n",
    "            integer specifying how large should `max_time` dimension be.\n",
    "            If None, maximum sequence length would be used\n",
    "    \n",
    "    Outputs:\n",
    "        inputs_time_major:\n",
    "            input sentences transformed into time-major matrix \n",
    "            (shape [max_time, batch_size]) padded with 0s\n",
    "        sequence_lengths:\n",
    "            batch-sized list of integers specifying amount of active \n",
    "            time steps in each input sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    sequence_lengths = [len(seq) for seq in inputs]\n",
    "    batch_size = len(inputs)\n",
    "    \n",
    "    if max_sequence_length is None:\n",
    "        max_sequence_length = max(sequence_lengths)\n",
    "    \n",
    "    inputs_batch_major = np.zeros(shape=[batch_size, max_sequence_length], dtype=np.int32) # == PAD\n",
    "    \n",
    "    for i, seq in enumerate(inputs):\n",
    "        for j, element in enumerate(seq):\n",
    "            inputs_batch_major[i, j] = element\n",
    "\n",
    "    # [batch_size, max_time] -> [max_time, batch_size]\n",
    "    inputs_time_major = inputs_batch_major.swapaxes(0, 1)\n",
    "\n",
    "    return inputs_time_major, sequence_lengths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def random_sequences(length_from, length_to,\n",
    "                     vocab_lower, vocab_upper,\n",
    "                     batch_size):\n",
    "    \"\"\" Generates batches of random integer sequences,\n",
    "        sequence length in [length_from, length_to],\n",
    "        vocabulary in [vocab_lower, vocab_upper]\n",
    "    \"\"\"\n",
    "    if length_from > length_to:\n",
    "            raise ValueError('length_from > length_to')\n",
    "\n",
    "    def random_length():\n",
    "        if length_from == length_to:\n",
    "            return length_from\n",
    "        return np.random.randint(length_from, length_to + 1)\n",
    "    \n",
    "    while True:\n",
    "        yield [\n",
    "            np.random.randint(low=vocab_lower,\n",
    "                              high=vocab_upper,\n",
    "                              size=random_length()).tolist()\n",
    "            for _ in range(batch_size)\n",
    "]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(target, source, max_len, batch_size):\n",
    "    iterator, word2index, index2word = prepare_data(target, source, max_len, batch_size)\n",
    "\n",
    "    print('head of the batch:')\n",
    "\n",
    "    tgt_list, src_list = [], []\n",
    "    tgt_vocab, src_vocab = set(), set()\n",
    "\n",
    "    # switch src and tgt between Arabic and English\n",
    "    for (tgt, src) in iterator:\n",
    "        for w in src:\n",
    "            tgt_vocab.add(w)\n",
    "\n",
    "        for w in tgt:\n",
    "            src_vocab.add(w)\n",
    "\n",
    "        tgt_list.append(src)\n",
    "        src_list.append(tgt)\n",
    "        \n",
    "    return tgt_vocab, src_vocab, tgt_list, src_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in training files...\n",
      "Sentence number 0\n",
      "Sentence number 100000\n",
      "Sentence number 200000\n",
      "Sentence number 300000\n",
      "Sentence number 400000\n",
      "Sentence number 500000\n",
      "Sentence number 600000\n",
      "Sentence number 700000\n",
      "Sentence number 800000\n",
      "Sentence number 900000\n",
      "Sentence number 1000000\n",
      "Sentence number 1100000\n",
      "Creating vocabulary file...\n",
      "Indexing words...\n",
      "Prepare for bucketing...\n",
      "Create iterator...\n",
      "Generate batches...\n",
      "head of the batch:\n",
      "Reading in training files...\n",
      "Sentence number 0\n",
      "Creating vocabulary file...\n",
      "Indexing words...\n",
      "Prepare for bucketing...\n",
      "Create iterator...\n",
      "Generate batches...\n",
      "head of the batch:\n",
      "Reading in training files...\n",
      "Sentence number 0\n",
      "Creating vocabulary file...\n",
      "Indexing words...\n",
      "Prepare for bucketing...\n",
      "Create iterator...\n",
      "Generate batches...\n",
      "head of the batch:\n",
      "<class 'generator'>\n",
      "34314 55504\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read the prepare the data here\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "    \n",
    "tgt_vocab, src_vocab, tgt_list, src_list = get_data('train.bpe.eng', 'train.bpe.ara', 50, batch_size)\n",
    "dev_tgt_vocab, dev_src_vocab, dev_tgt_list, dev_src_list = get_data('dev.bpe.eng', 'dev.bpe.ara', 50, batch_size)\n",
    "test_tgt_vocab, test_src_vocab, test_tgt_list, test_src_list = get_data('test1.bpe.eng', 'test.bpe.ara', 50, batch_size)\n",
    "\n",
    "# partition lists into chuncks of size batch_size\n",
    "def make_chunks(dataset, n):\n",
    "    # accepts a list and an integer (n), yields n chuncks of the list\n",
    "    for i in range(0, len(dataset), n):\n",
    "        yield dataset[i:i + n]\n",
    "        \n",
    "chunked_src = make_chunks(src_list, batch_size)\n",
    "print(type(chunked_src))\n",
    "chunked_tgt = make_chunks(tgt_list, batch_size)\n",
    "\n",
    "print(len(src_vocab), len(tgt_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "vocab_size = len(src_vocab) + 2\n",
    "input_embedding_size = 64 # character length \n",
    "\n",
    "encoder_hidden_units = 64\n",
    "decoder_hidden_units = encoder_hidden_units * 2\n",
    "\n",
    "\n",
    "\n",
    "# input placeholders \n",
    "# this is the batch! a tensor of tf.int32\n",
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "\n",
    "# contains the length for each sequence in a batch, we will pad so have same lenght\n",
    "# if you don't want to pad, check out dynamic memory networks to input variable length sequence \n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_input_length')\n",
    "\n",
    "# target sequence for pairs in a batch\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# embeddings: a tensor of tf.float32\n",
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "\n",
    "# this looks interesting!\n",
    "# tf.nn.embedding_lookup takes single tensor representing the complete embeddings,\n",
    "# and  a tensor with type int32 containing the ids to be looked up in embeddings\n",
    "# returns a tensor with the same type as the tensors in embeddings\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# define encoder's forward and backward LSTM\n",
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple\n",
    "\n",
    "# define fw and bw cells \n",
    "with tf.variable_scope('forward'):\n",
    "    encoder_fw_cell = LSTMCell(encoder_hidden_units)\n",
    "with tf.variable_scope('backward'):\n",
    "    encoder_bw_cell = LSTMCell(encoder_hidden_units)\n",
    "    \n",
    "# initilize BRNN\n",
    "((encoder_fw_outputs,\n",
    " encoder_bw_outputs), \n",
    "(encoder_fw_final_state,\n",
    "encoder_bw_final_state)) = (\n",
    "    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_fw_cell,\n",
    "                                    cell_bw=encoder_bw_cell,\n",
    "                                    inputs=encoder_inputs_embedded,\n",
    "                                    sequence_length=encoder_inputs_length,\n",
    "                                    dtype=tf.float32, time_major=True)\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bidirectional step\n",
    "# concatenates tensors along one dimension\n",
    "# the resulting tensor of this concatenation has shape (?, 40)\n",
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 1)\n",
    "\n",
    "# h: hidden state, c: cell state\n",
    "encoder_final_state_c = tf.concat(\n",
    "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "# create an LSTMStateTuple object for the encoder's final state\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c = encoder_final_state_c,\n",
    "    h = encoder_final_state_h\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# decoder cell\n",
    "decoder_cell = LSTMCell(decoder_hidden_units)\n",
    "\n",
    "# tf.unstack returns the shapes of the tensor: max sequence length and batch size\n",
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))\n",
    "\n",
    "# length of the target sequence (in training)\n",
    "# NOTE: I am not sure why Siraj put +3 here!!!\n",
    "decoder_lengths = encoder_inputs_length # + 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Shape_1:0' shape=(2,) dtype=int32>,\n",
       " [<tf.Tensor 'unstack_1:0' shape=() dtype=int32>,\n",
       "  <tf.Tensor 'unstack_1:1' shape=() dtype=int32>],\n",
       " <tf.Tensor 'encoder_input_length:0' shape=(?,) dtype=int32>,\n",
       " (<tf.Tensor 'unstack:0' shape=() dtype=int32>,\n",
       "  <tf.Tensor 'unstack:1' shape=() dtype=int32>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Added by Badr\n",
    "# here, we should have something similar to encoder_inputs_embedding \n",
    "decoder_targets_embedded = tf.nn.embedding_lookup(embeddings, decoder_targets)\n",
    "\n",
    "#decoder_targets_embedded_ta = tf.TensorArray(dtype=tf.float32, size=encoder_inputs_length[0])\n",
    "#decoder_targets_embedded_ta = decoder_targets_embedded_ta.unstack(decoder_targets_embedded)\n",
    "\n",
    "\n",
    "tf.shape(encoder_inputs), tf.unstack(tf.shape(encoder_inputs)), decoder_lengths, (encoder_max_time, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# outputs projection\n",
    "# define our weights and biases \n",
    "# these weights correspond to the input matrix  \n",
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype=tf.float32)\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create padded inputs for the decoder from the word embeddings\n",
    "\n",
    "# were telling the program to test a condition, and trigger an error if the condition is false.\n",
    "assert EOS == 1 and PAD == 0\n",
    "\n",
    "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
    "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
    "\n",
    "# retrieves rows of the params tensor. The behavior is similar to using indexing with arrays in numpy\n",
    "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
    "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# manually specifying loop function through time - to get initial cell state and input to RNN\n",
    "# normally we'd just use dynamic_rnn, but lets get detailed here with raw_rnn\n",
    "\n",
    "# we define and return these values, no operations occur here\n",
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
    "    #end of sentence\n",
    "    initial_input = eos_step_embedded\n",
    "    # last time steps cell state\n",
    "    initial_cell_state = encoder_final_state\n",
    "    # none\n",
    "    initial_cell_output = None\n",
    "    # none\n",
    "    initial_loop_state = None  # we don't need to pass any additional information\n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# attention mechanism --choose which previously generated token to pass as input in the next timestep\n",
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "    \n",
    "    def get_next_input():\n",
    "        # dot product between previous ouput and weights, then + biases\n",
    "        \n",
    "        output_logits = tf.add(tf.matmul(previous_output, W), b)\n",
    "        # Logits simply means that the function operates on the unscaled output of \n",
    "        # earlier layers and that the relative scale to understand the units is linear. \n",
    "        # It means, in particular, the sum of the inputs may not equal 1, that the values are not probabilities \n",
    "        # (you might have an input of 5).\n",
    "        # prediction value at current time step\n",
    "        \n",
    "        # Returns the index with the largest value across axes of a tensor.\n",
    "        # This is attention!! Nope it is not\n",
    "        # This line should not be applied during training \n",
    "        # it would be possible to use this line during inference\n",
    "        # instead, the next_input should be the item in the ground truth data, not the predicted\n",
    "        # for teacher forcing (also known as MLE)\n",
    "        prediction = tf.argmax(output_logits, axis=1)\n",
    "\n",
    "        # embed prediction for the next input\n",
    "        next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
    "        return next_input \n",
    "    \n",
    "    elements_finished = (time >= decoder_lengths) # this operation produces boolean tensor of [batch_size]\n",
    "                                                  # defining if corresponding sequence has ended\n",
    "   \n",
    "    # Computes the \"logical and\" of elements across dimensions of a tensor.\n",
    "    finished = tf.reduce_all(elements_finished) # -> boolean scalar\n",
    "    \n",
    "    # Return either fn1() or fn2() based on the boolean predicate pred.\n",
    "    input = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "    #input = tf.cond(finished, lambda: pad_step_embedded, lambda: decoder_targets_embedded_ta.read(time))\n",
    "    \n",
    "    # set previous to current\n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "\n",
    "    return (elements_finished, \n",
    "            input,\n",
    "            state,\n",
    "            output,\n",
    "            loop_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    if previous_state is None:    # time == 0\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Creates an RNN specified by RNNCell cell and loop function loop_fn.\n",
    "#This function is a more primitive version of dynamic_rnn that provides more direct access to the \n",
    "#inputs each iteration. It also provides more control over when to start and finish reading the sequence, \n",
    "#and what to emit for the output.\n",
    "#ta = tensor array\n",
    "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
    "decoder_outputs = decoder_outputs_ta.stack()\n",
    "\n",
    "# to convert output to human readable prediction\n",
    "# we will reshape output tensor\n",
    "\n",
    "# Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors.\n",
    "# reduces dimensionality\n",
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "# flettened output tensor\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "# pass flattened tensor through decoder\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "# prediction vals\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# final prediction\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# cross entropy loss\n",
    "# one hot encode the target values so we don't rank just differentiate\n",
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "# train it \n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "loss_track = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def next_feed(batch):\n",
    "    (src_batch, tgt_batch) = batch\n",
    "    encoder_inputs_batch, encoder_input_lengths_ = make_batch(src_batch)\n",
    "    decoder_targets_, _ = make_batch(src_batch)\n",
    "\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_batch,\n",
    "        encoder_inputs_length: encoder_input_lengths_,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize graphs\n",
    "train_graph = tf.Graph()\n",
    "eval_graph = tf.Graph()\n",
    "infer_graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "epoch 0,  batch 0\n",
      "  minibatch loss: 10.558125495910645\n",
      "  sample 1:\n",
      "    input     > [48  0  0  0  0  0]\n",
      "    reference > [48  0  0  0  0  0]\n",
      "    predicted > [33339     0     0     0     0     0]\n",
      "  sample 2:\n",
      "    input     > [3280    0    0    0    0    0]\n",
      "    reference > [3280    0    0    0    0    0]\n",
      "    predicted > [9514    0    0    0    0    0]\n",
      "  sample 3:\n",
      "    input     > [21  0  0  0  0  0]\n",
      "    reference > [21  0  0  0  0  0]\n",
      "    predicted > [13949     0     0     0     0     0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "batches_in_epoch = len(src_list) // batch_size\n",
    "eval_batch = 100\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "try:\n",
    "    for epoch in range(epochs):\n",
    "        chunked_src = make_chunks(src_list, batch_size)\n",
    "        print(type(chunked_src))\n",
    "        chunked_tgt = make_chunks(tgt_list, batch_size)\n",
    "        chunked_dev_src = make_chunks(dev_src_list, batch_size)\n",
    "        chunked_dev_tgt = make_chunks(dev_tgt_list, batch_size)\n",
    "        for batch in range(2):\n",
    "            with train_graph.as_default():\n",
    "                fd = next_feed((next(chunked_src), next(chunked_tgt)))\n",
    "                #print(fd[encoder_inputs].shape)\n",
    "                #print(fd[decoder_targets].shape)\n",
    "                #print(fd[encoder_inputs])\n",
    "                #print(fd[decoder_targets])\n",
    "                _, l = sess.run([train_op, loss], fd)\n",
    "                loss_track.append(l)\n",
    "\n",
    "\n",
    "            if batch == 0 or batch % eval_batch == 0:\n",
    "                with eval_graph.as_default(): ########## TO HAMZA: This section is where you add the BLEU calculations\n",
    "                        eval_fd = next_feed((next(chunked_dev_src), next(chunked_dev_tgt)))\n",
    "                        print('epoch {},  batch {}'.format(epoch, batch))\n",
    "                        print('  minibatch loss: {}'.format(sess.run(loss, eval_fd)))\n",
    "                        predict_ = sess.run(decoder_prediction, eval_fd)\n",
    "                        for i, (inp, ref, pred) in enumerate(zip(eval_fd[encoder_inputs].T, eval_fd[decoder_targets].T, predict_.T)):\n",
    "                            print('  sample {}:'.format(i + 1))\n",
    "                            print('    input     > {}'.format(inp))\n",
    "                            print('    reference > {}'.format(ref))\n",
    "                            print('    predicted > {}'.format(pred))\n",
    "                            if i >= 2:\n",
    "                                break\n",
    "                        print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-0d218baa5180>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mchunked_test_tgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tgt_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches_in_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunked_test_src\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunked_test_tgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#this would actually have to be changed to reading in from eval.eng and batching it (using preproc_pipe.py)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mpredict_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nest' is not defined"
     ]
    }
   ],
   "source": [
    "with infer_graph.as_default():\n",
    "    batches_in_epoch = len(test_src_list) // batch_size\n",
    "    chunked_test_src = make_chunks(test_src_list, batch_size)\n",
    "    chunked_test_tgt = make_chunks(test_tgt_list, batch_size)\n",
    "    for batch in range(batches_in_epoch):\n",
    "        fd = next_feed((next(chunked_test_src), nest(chunked_test_tgt))) #this would actually have to be changed to reading in from eval.eng and batching it (using preproc_pipe.py)\n",
    "        predict_ = sess.run(decoder_prediction, fd)\n",
    "        for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "            print('  sample {}:'.format(i + 1))\n",
    "            print('    input     > {}'.format(inp))\n",
    "            print('    predicted > {}'.format(pred))\n",
    "            if i >= 2:\n",
    "                break\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
